{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7be6486-cb39-4e61-955d-6d7ec6aa9feb",
   "metadata": {},
   "source": [
    "# NetCLR Pre-training\n",
    "\n",
    "In this notebook, we perform the NetCLR pre-training phase using 100 websites of the AWF dataset. There are 500 superior traces for each website.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "769b630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "import tqdm\n",
    "import pickle\n",
    "import argparse\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import bisect\n",
    "\n",
    "import dill\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56ef7fd-1add-4d92-86c8-5e2cef5ea272",
   "metadata": {},
   "source": [
    "## GPU allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a932a363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\", 0)\n",
    "kwargs = {'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "print (f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522b80c5-cb64-4dd3-9ca2-8c28fdf58278",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c0e301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "fp16_precision = True\n",
    "temperature = 0.5\n",
    "n_views = 2\n",
    "num_epoches = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5495e455-972b-4d76-bfdd-c7a8d26444d6",
   "metadata": {},
   "source": [
    "## Load Superior AWF-PT dataset\n",
    "\n",
    "Here we load the AWF pre-training data. You can replace the .npz file to use your pre-training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "872be547-ee68-4917-814d-17d593a4bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/path/to/AWF-PT-sup/dataset/file') # AWF-PT-sup\n",
    "x_train = data['data']\n",
    "y_train = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01964d9b-9109-4974-b79d-69e348af4a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 100\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(np.unique(y_train))\n",
    "print (f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f29377b-1668-4487-ae61-c81b24833f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shapes: (50000, 5000), (50000,)\n"
     ]
    }
   ],
   "source": [
    "print (f'Train data shapes: {x_train.shape}, {y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a01f39",
   "metadata": {},
   "source": [
    "## Backbone Model\n",
    "\n",
    "The backbone of NetCLR model is the Deep Fingerprinting neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91cddc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFNet(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super(DFNet, self).__init__()\n",
    "        kernel_size = 8\n",
    "        channels = [1, 32, 64, 128, 256]\n",
    "        conv_stride = 1\n",
    "        pool_stride = 4\n",
    "        pool_size = 8\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size, stride = conv_stride)\n",
    "        self.conv1_1 = nn.Conv1d(32, 32, kernel_size, stride = conv_stride)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size, stride = conv_stride)\n",
    "        self.conv2_2 = nn.Conv1d(64, 64, kernel_size, stride = conv_stride)\n",
    "       \n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size, stride = conv_stride)\n",
    "        self.conv3_3 = nn.Conv1d(128, 128, kernel_size, stride = conv_stride)\n",
    "       \n",
    "        self.conv4 = nn.Conv1d(128, 256, kernel_size, stride = conv_stride)\n",
    "        self.conv4_4 = nn.Conv1d(256, 256, kernel_size, stride = conv_stride)\n",
    "       \n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(32)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(64)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(128)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.max_pool_1 = nn.MaxPool1d(kernel_size=pool_size, stride=pool_stride)\n",
    "        self.max_pool_2 = nn.MaxPool1d(kernel_size=pool_size, stride=pool_stride)\n",
    "        self.max_pool_3 = nn.MaxPool1d(kernel_size=pool_size, stride=pool_stride)\n",
    "        self.max_pool_4 = nn.MaxPool1d(kernel_size=pool_size, stride=pool_stride)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.dropout2 = nn.Dropout(p=0.1)\n",
    "        self.dropout3 = nn.Dropout(p=0.1)\n",
    "        self.dropout4 = nn.Dropout(p=0.1)\n",
    "\n",
    "        \n",
    "        self.fc = nn.Linear(5120, out_dim)\n",
    "\n",
    "        \n",
    "    def weight_init(self):\n",
    "        for n, m in self.named_modules():\n",
    "            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):\n",
    "#                 m.weight.data.xavier_uniform_()\n",
    "                # print (n)\n",
    "                torch.nn.init.xavier_uniform(m.weight)\n",
    "                m.bias.data.zero_()\n",
    "            \n",
    "        \n",
    "    def forward(self, inp):\n",
    "        x = inp\n",
    "        # ==== first block ====\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.elu((self.conv1(x)))\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.elu(self.batch_norm1(self.conv1_1(x)))\n",
    "#         x = F.elu(self.conv1_1(x))\n",
    "        x = F.pad(x, (3, 4))\n",
    "        x = self.max_pool_1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # ==== second block ====\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.relu((self.conv2(x)))\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.relu(self.batch_norm2(self.conv2_2(x)))\n",
    "#         x = F.relu(self.conv2_2(x))\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = self.max_pool_2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # ==== third block ====\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.relu((self.conv3(x)))\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.relu(self.batch_norm3(self.conv3_3(x)))\n",
    "#         x = F.relu(self.conv3_3(x))\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = self.max_pool_3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # ==== fourth block ====\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.relu((self.conv4(x)))\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.relu(self.batch_norm4(self.conv4_4(x)))\n",
    "#         x = F.relu(self.conv4_4(x))\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = self.max_pool_4(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "                \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "#         x = self.projection(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "                \n",
    "        return x    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af424ed4-c95d-44e6-908c-788a522406b6",
   "metadata": {},
   "source": [
    "## Adding Projection Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "372e89e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFsimCLR(nn.Module):\n",
    "    def __init__(self, df, out_dim):\n",
    "        super(DFsimCLR, self).__init__()\n",
    "        \n",
    "        self.backbone = df\n",
    "        self.backbone.weight_init()\n",
    "        dim_mlp = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Linear(dim_mlp, dim_mlp),\n",
    "            nn.BatchNorm1d(dim_mlp),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_mlp, out_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        out = self.backbone(inp)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761bfd6d",
   "metadata": {},
   "source": [
    "## NetAugment\n",
    "\n",
    "This part shows the implementation of NetAugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12bade7c-37c1-41d5-a9e3-320590cf38f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bursts(x):\n",
    "    \n",
    "    direction = x[0]\n",
    "    bursts = []\n",
    "    start = 0\n",
    "    temp_burst = x[0]\n",
    "    for i in range(1, len(x)):\n",
    "        if x[i] == 0.0:\n",
    "            break\n",
    "        \n",
    "        elif x[i] == direction:\n",
    "            temp_burst += x[i]\n",
    "            \n",
    "        else:\n",
    "            # if temp_burst <= -10 or temp_burst > 0:\n",
    "            bursts.append((start, i, temp_burst))\n",
    "            start = i\n",
    "            temp_burst = x[i]\n",
    "            direction *= -1\n",
    "            \n",
    "    return bursts\n",
    "\n",
    "outgoing_burst_sizes = []\n",
    "\n",
    "x_random = x_train[np.random.choice(range(len(x_train)), size=1000, replace=False)]\n",
    "\n",
    "\n",
    "\n",
    "for x in x_random:\n",
    "    bursts = find_bursts(x)\n",
    "    \n",
    "    outgoing_burst_sizes += [x[2] for x in bursts if x[2] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a78ef088",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_outgoing_burst_size = max(outgoing_burst_sizes)\n",
    "# len(outgoing_burst_sizes), min(outgoing_burst_sizes), max(outgoing_burst_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58030065-5253-4ed7-abf9-b2920c41b6c0",
   "metadata": {},
   "source": [
    "### Empirical Distribution of Outgoing Bursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbf9d986-27a3-4893-ba84-204adc141f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "count, bins = np.histogram(outgoing_burst_sizes, bins=max_outgoing_burst_size - 1)\n",
    "PDF = count/np.sum(count)\n",
    "OUTGOING_BURST_SIZE_CDF = np.zeros_like(bins)\n",
    "OUTGOING_BURST_SIZE_CDF[1:] = np.cumsum(PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b8f25be-cba7-4b21-8c05-118b55c8182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmentor():\n",
    "    def __init__(self):\n",
    "        methods = {\n",
    "            'merge downstream burst',\n",
    "            'change downstream burst sizes',\n",
    "            'merge downstream and upstream bursts',\n",
    "            'add upstream bursts',\n",
    "            'remove upstrean bursts',\n",
    "            'divide bursts'\n",
    "        }\n",
    "        \n",
    "        \n",
    "        self.large_burst_threshold = 10\n",
    "        \n",
    "        # changing the content\n",
    "        self.upsample_rate = 1.0\n",
    "        self.downsample_rate = 0.5\n",
    "        \n",
    "        # merging bursts\n",
    "        self.num_bursts_to_merge = 5\n",
    "        self.merge_burst_rate = 0.1\n",
    "        \n",
    "        # add incoming bursts\n",
    "        self.add_outgoing_burst_rate = 0.3\n",
    "        self.outgoing_burst_sizes = list(range(max_outgoing_burst_size))\n",
    "        \n",
    "        # shift\n",
    "        self.shift_param = 10\n",
    "        \n",
    "        \n",
    "        \n",
    "    def find_bursts(self, x):\n",
    "        direction = x[0]\n",
    "        bursts = []\n",
    "        start = 0\n",
    "        temp_burst = x[0]\n",
    "        for i in range(1, len(x)):\n",
    "            if x[i] == 0.0:\n",
    "                break\n",
    "\n",
    "            elif x[i] == direction:\n",
    "                temp_burst += x[i]\n",
    "\n",
    "            else:\n",
    "                # if temp_burst <= -10 or temp_burst > 0:\n",
    "                bursts.append((start, i, temp_burst))\n",
    "                start = i\n",
    "                temp_burst = x[i]\n",
    "                direction *= -1\n",
    "\n",
    "        return bursts\n",
    "        \n",
    "        \n",
    "    # representing the change of contents of a website\n",
    "    def increase_incoming_bursts(self, burst_sizes):\n",
    "        out = []\n",
    "        for i, size in enumerate(burst_sizes):\n",
    "            if size <= -self.large_burst_threshold:\n",
    "                up_sample_rate = random.random()*self.upsample_rate\n",
    "                new_size = int(size * (1+up_sample_rate))\n",
    "                out.append(new_size)\n",
    "            else:\n",
    "                out.append(size)\n",
    "                \n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def decrease_incoming_bursts(self, burst_sizes):\n",
    "        out = []\n",
    "        for i, size in enumerate(burst_sizes):\n",
    "            if size <= -self.large_burst_threshold:\n",
    "                up_sample_rate = random.random()*self.downsample_rate\n",
    "                new_size = int(size * (1-up_sample_rate))\n",
    "                out.append(new_size)\n",
    "            else:\n",
    "                out.append(size)\n",
    "                \n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def change_content(self, trace):\n",
    "        bursts = self.find_bursts(trace)\n",
    "        burst_sizes = [x[2] for x in bursts]\n",
    "        \n",
    "        if len(trace) < 1000:\n",
    "            new_burst_sizes = self.increase_incoming_bursts(burst_sizes)\n",
    "            \n",
    "        elif len(trace) > 4000:\n",
    "            new_burst_sizes = self.decrease_incoming_bursts(burst_sizes)\n",
    "            \n",
    "        else:\n",
    "            p = random.random()\n",
    "            if p >= 0.5:\n",
    "                new_burst_sizes = self.increase_incoming_bursts(burst_sizes)\n",
    "                \n",
    "            else:\n",
    "                new_burst_sizes = self.decrease_incoming_bursts(burst_sizes)\n",
    "                \n",
    "                \n",
    "        return new_burst_sizes\n",
    "    \n",
    "    \n",
    "    def merge_incoming_bursts(self, burst_sizes):\n",
    "        \n",
    "        out = []\n",
    "        \n",
    "        # skipping first 20 cells\n",
    "        i = 0\n",
    "        num_cells = 0\n",
    "        while i < len(burst_sizes) and num_cells < 20:\n",
    "            num_cells += abs(burst_sizes[i])\n",
    "            out.append(burst_sizes[i])\n",
    "            i += 1\n",
    "            \n",
    "        \n",
    "        while i < len(burst_sizes) - self.num_bursts_to_merge:\n",
    "            prob = random.random()\n",
    "            \n",
    "            # ignore outgoing bursts\n",
    "            if burst_sizes[i] > 0:\n",
    "                out.append(burst_sizes[i])\n",
    "                i+= 1\n",
    "                continue\n",
    "            \n",
    "            if prob < self.merge_burst_rate:\n",
    "                num_merges = random.randint(2, self.num_bursts_to_merge)\n",
    "                merged_size = 0\n",
    "                \n",
    "                # merging the incoming bursts\n",
    "                while i < len(burst_sizes) and num_merges > 0:\n",
    "                    if burst_sizes[i] < 0:\n",
    "                        merged_size += burst_sizes[i]\n",
    "                        num_merges -= 1\n",
    "                    i += 1     \n",
    "                out.append(merged_size)\n",
    "                    \n",
    "            else:\n",
    "                out.append(burst_sizes[i])\n",
    "                i += 1\n",
    "                \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def add_outgoing_burst(self, burst_sizes):\n",
    "        \n",
    "        out = []\n",
    "        \n",
    "        i = 0\n",
    "        num_cells = 0\n",
    "        while i < len(burst_sizes) and num_cells < 20:\n",
    "            num_cells += abs(burst_sizes[i])\n",
    "            out.append(burst_sizes[i])\n",
    "            i += 1\n",
    "            \n",
    "        \n",
    "        for size in burst_sizes[i:]:\n",
    "            if size > -10 :\n",
    "                out.append(size)\n",
    "                continue\n",
    "            \n",
    "            prob = random.random()\n",
    "            \n",
    "            if prob < self.add_outgoing_burst_rate:\n",
    "                \n",
    "                index = len(outgoing_burst_sizes)\n",
    "                while index >= len(outgoing_burst_sizes):\n",
    "                    outgoing_burst_prob = random.random()\n",
    "                    index = bisect.bisect_left(OUTGOING_BURST_SIZE_CDF, outgoing_burst_prob)\n",
    "                    \n",
    "                outgoing_burst_size = self.outgoing_burst_sizes[index]\n",
    "                divide_place = random.randint(3, abs(size) - 3)\n",
    "                \n",
    "                out += [-divide_place, outgoing_burst_size, -(abs(size) - divide_place)]\n",
    "                \n",
    "            else:\n",
    "                out.append(size)\n",
    "                \n",
    "        return out\n",
    "                \n",
    "        \n",
    "    def create_trace_from_burst_sizes(self, burst_sizes):\n",
    "        out = []\n",
    "        \n",
    "        for size in burst_sizes:\n",
    "            val = 1 if size > 0 else -1\n",
    "            \n",
    "            out += [val]*(int(abs(size)))\n",
    "            \n",
    "        if len(out) < 5000:\n",
    "            out += [0]*(5000 - len(out))\n",
    "            \n",
    "        return np.array(out)[:5000]\n",
    "    \n",
    "    def shift(self, x):\n",
    "        pad = np.random.randint(0, 2, size = (self.shift_param, ))\n",
    "        pad = 2*pad-1\n",
    "        zpad = np.zeros_like(pad)\n",
    "        \n",
    "        shift_val = np.random.randint(-self.shift_param, self.shift_param+1, 1)[0]\n",
    "        shifted = np.concatenate((x, zpad, pad), axis=-1)\n",
    "        shifted = np.roll(shifted, shift_val, axis=-1)\n",
    "        shifted = shifted[:5000]\n",
    "        \n",
    "        return shifted\n",
    "        \n",
    "    \n",
    "    def augment(self, trace):\n",
    "        \n",
    "        mapping = {\n",
    "            0: self.change_content,\n",
    "            1: self.merge_incoming_bursts,\n",
    "            2: self.add_outgoing_burst\n",
    "        }\n",
    "        \n",
    "        bursts = self.find_bursts(trace)\n",
    "        \n",
    "        burst_sizes = [x[2] for x in bursts]\n",
    "        \n",
    "        \n",
    "        aug_method = mapping[random.randint(0, len(mapping)-1)]\n",
    "        \n",
    "        augmented_sizes = aug_method(burst_sizes)\n",
    "        \n",
    "        augmented_trace = self.create_trace_from_burst_sizes(augmented_sizes)\n",
    "        \n",
    "        return self.shift(augmented_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6867443",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8b91c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(Dataset):\n",
    "    def __init__(self, x_train, y_train, augmentor, n_views):\n",
    "        self.x = x_train\n",
    "        self.y = y_train\n",
    "        self.augmentor = augmentor\n",
    "        self.n_views = n_views\n",
    "    \n",
    "    def _aug(self, inp):\n",
    "        flip_idx = np.random.randint(0, 4999, 250)\n",
    "        x_w = inp.copy()\n",
    "        temp = x_w[flip_idx]\n",
    "        x_w[flip_idx] = x_w[flip_idx+1]\n",
    "        x_w[flip_idx+1] = temp\n",
    "        return x_w\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return [self.augmentor.augment(self.x[index]) for i in range(self.n_views)], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47786995",
   "metadata": {},
   "source": [
    "## NetCLR Class\n",
    "\n",
    "Here is the NetCLR class that performs the augmentation and NCE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b25ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc9bb07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetCLR(object):\n",
    "    def __init__(self, **args):\n",
    "        self.model = args['model']\n",
    "        self.optimizer = args['optimizer']\n",
    "        self.scheduler = args['scheduler']\n",
    "        self.fp16_precision = args['fp16_precision']\n",
    "        self.num_epoches = args['num_epoches']\n",
    "        self.batch_size = args['batch_size']\n",
    "        self.device = args['device']\n",
    "        self.temperature = args['temperature']\n",
    "#         self.tester = args['tester']\n",
    "        self.n_views = 2\n",
    "        self.criterion = torch.nn.CrossEntropyLoss().to(self.device)\n",
    "        self.log_every_n_step = 100\n",
    "    \n",
    "    def info_nce_loss(self, features):\n",
    "        labels = torch.cat([torch.arange(self.batch_size) for i in range(self.n_views)], dim = 0)\n",
    "        labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n",
    "        labels = labels.to(self.device)\n",
    "        \n",
    "        features = F.normalize(features, dim=1)\n",
    "        \n",
    "        similarity_matrix = torch.matmul(features, features.T)\n",
    "        \n",
    "        mask = torch.eye(labels.shape[0], dtype=torch.bool).to(self.device)\n",
    "        labels = labels[~mask].view(labels.shape[0], -1)\n",
    "        similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)\n",
    "        \n",
    "        positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1)\n",
    "        \n",
    "        \n",
    "        negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1)\n",
    "        \n",
    "        \n",
    "        logits = torch.cat([positives, negatives], dim=1)\n",
    "        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(self.device)\n",
    "        \n",
    "        logits = logits / self.temperature\n",
    "        return logits, labels\n",
    "        \n",
    "    def train(self, train_loader):\n",
    "        best_acc = 0\n",
    "        scaler = GradScaler(enabled=self.fp16_precision)\n",
    "\n",
    "        n_iter = 0\n",
    "        print (\"Start SimCLR training for %d number of epoches\"%self.num_epoches)\n",
    "        \n",
    "        first_loss = True\n",
    "        for epoch_counter in range(self.num_epoches+1):\n",
    "            \n",
    "#             print (\"Epoch: \", epoch_counter)\n",
    "            with tqdm.tqdm(train_loader, unit='batch') as tepoch:\n",
    "                for data, _ in tepoch:\n",
    "                    tepoch.set_description(f\"Epoch {epoch_counter}\")\n",
    "                    \n",
    "                    model.train()\n",
    "                    data = torch.cat(data, dim = 0)\n",
    "                    data = data.view(data.size(0), 1, data.size(1))\n",
    "                    data = data.float().to(self.device)\n",
    "\n",
    "                    with autocast(enabled=self.fp16_precision):\n",
    "                        features = self.model(data)\n",
    "                        logits, labels = self.info_nce_loss(features)\n",
    "                        loss = self.criterion(logits, labels)\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    \n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(self.optimizer)\n",
    "                    scaler.update()\n",
    "                    \n",
    "                    if n_iter%self.log_every_n_step == 0:\n",
    "                        top1, top5 = accuracy(logits, labels, topk=(1, 5))\n",
    "                        tepoch.set_postfix(loss=loss.item(), accuracy = top1.item())\n",
    "                    n_iter += 1\n",
    "\n",
    "            if epoch_counter >= 10:\n",
    "                self.scheduler.step()\n",
    "            \n",
    "            # saving the model each \n",
    "            if epoch_counter % 20 == 0:\n",
    "                torch.save(self.model.state_dict(), f'./../models/NetCLR/NetCLR_epoch_{epoch_counter}.pth.tar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf49945-7671-4b34-8421-4209d5512d50",
   "metadata": {},
   "source": [
    "## Running the Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "007f47f7-a7c5-48a7-9501-9b8598ce9084",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.5 # this value is suggested by the original SimCLR paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8952263e-7e25-4056-9803-d1e45e502240",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor = Augmentor()\n",
    "\n",
    "train_dataset = TrainData(x_train, y_train, augmentor, 2)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "df = DFNet(out_dim=512)\n",
    "model = DFsimCLR(df, out_dim=128).to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003) #, weight_decay = 1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader), eta_min=0, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b617a05-dd44-4000-a43e-4cb05e179c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "netclr = NetCLR(model = model,\n",
    "               optimizer = optimizer,\n",
    "               scheduler = scheduler,\n",
    "               fp16_precision = fp16_precision,\n",
    "               device = device,\n",
    "               temperature = temperature,\n",
    "               n_views = n_views,\n",
    "               num_epoches = 401,\n",
    "               batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aacec0e-6e06-407c-bf8d-5fb233769ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "netclr.train(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a59a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
