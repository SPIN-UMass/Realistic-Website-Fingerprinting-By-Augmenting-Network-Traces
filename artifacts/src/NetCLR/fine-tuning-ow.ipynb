{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5391bb7-0947-4a29-8748-794853fa7663",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NetCLR Fine-tuning \n",
    "\n",
    "In this notebook, we fine-tune the pre-trained base model of NetCLR in a open world scenario. \n",
    "\n",
    "We evaluate NetCLR using two datasets: AWF and Drift datasets. \n",
    "\n",
    "N defines the number of labeled samples that we use for fine-tuning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d826385b-0f7b-45a9-89c7-9446392b0128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "# from torchvision import datasets, transforms\n",
    "import tqdm\n",
    "\n",
    "import pickle\n",
    "import argparse\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfc54b0-2f51-4c46-8bba-3d1c3e0a9f76",
   "metadata": {},
   "source": [
    "## GPU Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9e40b61-b681-4246-afbf-68c4e7b57169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\", 0)\n",
    "kwargs = {'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "print (f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76859ee7-c19b-4183-b228-b82203356a4d",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33608c92-641e-465d-a50d-5540d779eb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2532b8-cc46-47c3-8545-a7b3c99a6633",
   "metadata": {},
   "source": [
    "## Helper Functions \n",
    "\n",
    "Helper functions to sample labeled traces randomly from both closed world and open world datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6a025fd-0a1b-4b15-8ee5-f94f01bcbfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_traces(x, y, N):\n",
    "    train_index = []\n",
    "    \n",
    "    for c in range(num_classes-1):\n",
    "        idx = np.where(y == c)[0]\n",
    "        idx = np.random.choice(idx, min(N, len(idx)), False)\n",
    "        train_index.extend(idx)\n",
    "        \n",
    "    train_index = np.array(train_index)\n",
    "    np.random.shuffle(train_index)\n",
    "    \n",
    "    x_train = x[train_index]\n",
    "    y_train = y[train_index]\n",
    "    \n",
    "    return x_train, y_train\n",
    "\n",
    "def sample_ow_traces(x, N, num_classes):\n",
    "    idx = np.random.randint(0, len(x), size=N*num_classes)\n",
    "    \n",
    "    return x[idx]## Helper Functions \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b322d70d-a8ab-420c-b3e8-4039e3f65b69",
   "metadata": {},
   "source": [
    "## Loading Closed World Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3602f3f3-2026-4f7f-aa2b-dbadc618e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'Drift'\n",
    "\n",
    "if DATASET == 'AWF':\n",
    "    data_path = '/path/to/AWF/closed-world-data/' # AWF-Attack\n",
    "    data = pickle.load(open(f'{data_path}', 'rb'))\n",
    "elif DATASET == 'Drift':\n",
    "    data_path = '/path/to/Drift/closed-world-data/'\n",
    "    data = pickle.load(open(f'{data_path}', 'rb')) # Drift90\n",
    "\n",
    "x_cw_train_total = data['x_train']\n",
    "y_cw_train_total = data['y_train'] \n",
    "x_cw_test_sup = data['x_test_fast']\n",
    "y_cw_test_sup = data['y_test_fast']\n",
    "x_cw_test_inf = data ['x_test_slow']\n",
    "y_cw_test_inf = data['y_test_slow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8d923b4-070a-42f8-b967-bc9c5deab47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed world dataset shapes: (9241, 5000), (1860, 5000), (1860, 5000)\n"
     ]
    }
   ],
   "source": [
    "print (f'Closed world dataset shapes: {x_cw_train_total.shape}, {x_cw_test_sup.shape}, {x_cw_test_inf.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "996aea98-caec-4dbe-a3a1-5e158403d616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes: 94\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(np.unique(y_cw_train_total)) + 1 # adding 1 for the open-world\n",
    "print (f\"Number of Classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb054bae-20cb-4b8a-b01d-2d87d3a48c8c",
   "metadata": {},
   "source": [
    "## Loading Open World Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "439d9b6f-1571-4bb3-af52-3d9ffa0a6ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'AWF':\n",
    "    data = np.load('/path/to/AWF/open-world-data/') # AWF-OW\n",
    "if DATASET == 'Drift':\n",
    "    data = np.load('/path/to/Drift/open-world-data/') # Drift5000\n",
    "\n",
    "x_ow_train = data['superior_train']\n",
    "x_ow_test_sup = data['inferior_test']\n",
    "x_ow_test_inf = data['inferior_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c828187d-39e4-4f8c-8cef-490c6e562b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open world dataset shapes: (1000, 5000), (4051, 5000), (4051, 5000)\n"
     ]
    }
   ],
   "source": [
    "print (f'Open world dataset shapes: {x_ow_train.shape}, {x_ow_test_sup.shape}, {x_ow_test_inf.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b6f2a4-905c-45d2-be25-1936f7806860",
   "metadata": {},
   "source": [
    "## Combine CW and OW test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91fb94ca-ac5d-46e7-b081-1ba62744e931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5911, 5000), (5911,), (5911, 5000), (5911,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cw_test_sup += 1\n",
    "y_cw_test_inf += 1\n",
    "\n",
    "x_test_sup = np.vstack((x_cw_test_sup, x_ow_test_sup))\n",
    "x_test_inf = np.vstack((x_cw_test_inf, x_ow_test_inf))\n",
    "\n",
    "y_ow_test = np.zeros((len(x_ow_test_inf), ))\n",
    "\n",
    "y_test_sup = np.hstack((y_cw_test_sup, y_ow_test))\n",
    "y_test_inf = np.hstack((y_cw_test_inf, y_ow_test))\n",
    "\n",
    "x_test_inf.shape, y_test_inf.shape, x_test_sup.shape, y_test_sup.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8c4ee0-5eaa-41a0-8d66-0290b19295bf",
   "metadata": {},
   "source": [
    "## Backbone Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f1ed09c-eee6-4829-b2d9-1e1e6f715e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFNet(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super(DFNet, self).__init__()\n",
    "        kernel_size = 8\n",
    "        channels = [1, 32, 64, 128, 256]\n",
    "        conv_stride = 1\n",
    "        pool_stride = 4\n",
    "        pool_size = 8\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size, stride = conv_stride)\n",
    "        self.conv1_1 = nn.Conv1d(32, 32, kernel_size, stride = conv_stride)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size, stride = conv_stride)\n",
    "        self.conv2_2 = nn.Conv1d(64, 64, kernel_size, stride = conv_stride)\n",
    "       \n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size, stride = conv_stride)\n",
    "        self.conv3_3 = nn.Conv1d(128, 128, kernel_size, stride = conv_stride)\n",
    "       \n",
    "        self.conv4 = nn.Conv1d(128, 256, kernel_size, stride = conv_stride)\n",
    "        self.conv4_4 = nn.Conv1d(256, 256, kernel_size, stride = conv_stride)\n",
    "       \n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(32)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(64)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(128)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.max_pool_1 = nn.MaxPool1d(kernel_size=pool_size, stride=pool_stride)\n",
    "        self.max_pool_2 = nn.MaxPool1d(kernel_size=pool_size, stride=pool_stride)\n",
    "        self.max_pool_3 = nn.MaxPool1d(kernel_size=pool_size, stride=pool_stride)\n",
    "        self.max_pool_4 = nn.MaxPool1d(kernel_size=pool_size, stride=pool_stride)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.dropout2 = nn.Dropout(p=0.1)\n",
    "        self.dropout3 = nn.Dropout(p=0.1)\n",
    "        self.dropout4 = nn.Dropout(p=0.1)\n",
    "        \n",
    "\n",
    "        \n",
    "        self.fc = nn.Linear(5120, out_dim)\n",
    "\n",
    "        \n",
    "    def weight_init(self):\n",
    "        for n, m in self.named_modules():\n",
    "            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):\n",
    "                print (n)\n",
    "                torch.nn.init.xavier_uniform(m.weight)\n",
    "                m.bias.data.zero_()\n",
    "            \n",
    "        \n",
    "    def forward(self, inp):\n",
    "        x = inp\n",
    "        # ==== first block ====\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.elu((self.conv1(x)))\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.elu(self.batch_norm1(self.conv1_1(x)))\n",
    "        x = F.pad(x, (3, 4))\n",
    "        x = self.max_pool_1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # ==== second block ====\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.relu((self.conv2(x)))\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.relu(self.batch_norm2(self.conv2_2(x)))\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = self.max_pool_2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # ==== third block ====\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.relu((self.conv3(x)))\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.relu(self.batch_norm3(self.conv3_3(x)))\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = self.max_pool_3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # ==== fourth block ====\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.relu((self.conv4(x)))\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.relu(self.batch_norm4(self.conv4_4(x)))\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = self.max_pool_4(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "                \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "\n",
    "\n",
    "        x = self.fc(x)\n",
    "                \n",
    "        return x    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c668e55-ad76-43d3-99f8-afbc78ce1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFsimCLR(nn.Module):\n",
    "    def __init__(self, df, out_dim):\n",
    "        super(DFsimCLR, self).__init__()\n",
    "        \n",
    "        self.backbone = df\n",
    "        self.backbone.weight_init()\n",
    "        dim_mlp = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Linear(dim_mlp, dim_mlp),\n",
    "            nn.BatchNorm1d(dim_mlp),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_mlp, out_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        out = self.backbone(inp)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3049cc27-cc5c-47bd-b3ee-e09b7de19656",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9b41628-1932-4d2a-9877-bc34a2c722f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a114a97-3990-42a7-888f-9d270b1cd285",
   "metadata": {},
   "source": [
    "## Loading the Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13f709d2-641f-49e2-aae2-bc4086440d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint():\n",
    "\n",
    "    model = DFNet(out_dim=num_classes).to(device)\n",
    "\n",
    "    checkpoint = torch.load('/path/to/pre-trained/model/')\n",
    "\n",
    "\n",
    "    for k in list(checkpoint.keys()):\n",
    "        if k.startswith('backbone.'):\n",
    "            if k.startswith('backbone') and not k.startswith('backbone.fc'):\n",
    "          # remove prefix\n",
    "                checkpoint[k[len(\"backbone.\"):]] = checkpoint[k]\n",
    "        del checkpoint[k]\n",
    "\n",
    "    log = model.load_state_dict(checkpoint, strict=False)\n",
    "    assert log.missing_keys == ['fc.weight', 'fc.bias']\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57015f2f-961f-46d7-9c99-9f5fdefb3c69",
   "metadata": {},
   "source": [
    "## Functions for Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a529697e-4cb5-4ce7-b86f-3c2a45f421c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.view(data.size(0), 1, data.size(1)).float().to(device)\n",
    "        target = target.type(torch.LongTensor)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # print (output.size())\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx%100 == 0:\n",
    "            print (\"Loss: {:0.6f}\".format(loss.item()))\n",
    "    \n",
    "def test(model, device, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    temp = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data = data.view(data.size(0), 1, data.size(1)).float().to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            output = torch.softmax(output, dim=1)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).float().sum().item()\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e27d2-7d5f-433e-b6e4-37496ac5d4f0",
   "metadata": {},
   "source": [
    "## Function for Open World Evaluation\n",
    "\n",
    "This function calculates the precision and recall of the WF classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc046da6-dd2a-4d5a-9ad3-cf1c80ea5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ow(model, device, loader, threshold, num_ow_test_samples):\n",
    "    \n",
    "    ow_label = 0\n",
    "    TP, FP, TN, FN, total = 0, 0, 0, 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target, in loader:\n",
    "            data = data.view(data.size(0), 1, data.size(1)).float().to(device)\n",
    "            target = target.detach().numpy() \n",
    "            \n",
    "            \n",
    "            total += len(data)\n",
    "            \n",
    "            output = model(data)\n",
    "            output = torch.softmax(output, dim=1)\n",
    "            output = output.cpu().detach().numpy()           \n",
    "            for pred, label in zip(output, target):\n",
    "                best_n = np.argmax(pred)\n",
    "                \n",
    "                # monitored websites\n",
    "                if int(label) != ow_label:\n",
    "                    if int(best_n) != ow_label:\n",
    "                        if pred[best_n] >= threshold:\n",
    "                            TP += 1\n",
    "                        else:\n",
    "                            FN += 1\n",
    "                            \n",
    "                    else:\n",
    "                        FN += 1\n",
    "                    \n",
    "                    \n",
    "                elif int(label) == ow_label:\n",
    "                    \n",
    "                    if int(best_n) != ow_label:\n",
    "                        if pred[best_n] >= threshold:\n",
    "                            FP += 1\n",
    "                        else:\n",
    "                            TN += 1\n",
    "                    else:\n",
    "                        TN += 1\n",
    "                        \n",
    "                        \n",
    "        return TP / (TP + FP), TP / (total - num_ow_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e93b9f-c5e2-4444-b50c-46baa30b4362",
   "metadata": {},
   "source": [
    "## Initiating Test Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a7cdf9d-a205-4692-bdeb-706271a991a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_inf = Data(x_test_inf, y_test_inf)\n",
    "test_loader_inf = DataLoader(test_dataset_inf, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "test_dataset_sup = Data(x_test_sup, y_test_sup)\n",
    "test_loader_sup = DataLoader(test_dataset_sup, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37958b6c-03a1-4499-9f80-81726511e92f",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc404579-a4f3-4982-b23f-45f73a155dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19f75b55-7dba-486b-8eae-86fdca11f740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed World: (465, 5000), (465,)\n",
      "Open World: (1000, 5000), (1000,)\n",
      "Total: (1465, 5000), (1465,)\n"
     ]
    }
   ],
   "source": [
    "x_cw_train, y_cw_train = sample_traces(x_cw_train_total, y_cw_train_total, N)\n",
    "y_cw_train += 1\n",
    "\n",
    "print (f'Closed World: {x_cw_train.shape}, {y_cw_train.shape}')\n",
    "\n",
    "# x_ow_train = sample_ow_traces(x_ow_train_total, N, num_classes-1)\n",
    "y_ow_train = np.zeros((len(x_ow_train), ))\n",
    "\n",
    "print (f'Open World: {x_ow_train.shape}, {y_ow_train.shape}')\n",
    "\n",
    "\n",
    "x_train = np.vstack((x_cw_train, x_ow_train))\n",
    "y_train = np.hstack((y_cw_train, y_ow_train))\n",
    "\n",
    "print (f'Total: {x_train.shape}, {y_train.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "431a12f4-63ea-4853-bd67-0ea372a575de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Data(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "model = load_checkpoint()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c503110c-7469-4398-afff-53c2899b1fa8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.700192\n",
      "-------------- Epoch 0 --------------\n",
      "Loss: 1.405838\n",
      "-------------- Epoch 1 --------------\n",
      "Loss: 1.632425\n",
      "-------------- Epoch 2 --------------\n",
      "Loss: 0.952068\n",
      "-------------- Epoch 3 --------------\n",
      "Loss: 0.873310\n",
      "-------------- Epoch 4 --------------\n",
      "Loss: 0.512898\n",
      "-------------- Epoch 5 --------------\n",
      "Loss: 0.289532\n",
      "-------------- Epoch 6 --------------\n",
      "Loss: 0.166581\n",
      "-------------- Epoch 7 --------------\n",
      "Loss: 0.104868\n",
      "-------------- Epoch 8 --------------\n",
      "Loss: 0.115181\n",
      "-------------- Epoch 9 --------------\n",
      "Loss: 0.057948\n",
      "-------------- Epoch 10 --------------\n",
      "Loss: 0.058850\n",
      "-------------- Epoch 11 --------------\n",
      "Loss: 0.026669\n",
      "-------------- Epoch 12 --------------\n",
      "Loss: 0.027162\n",
      "-------------- Epoch 13 --------------\n",
      "Loss: 0.029380\n",
      "-------------- Epoch 14 --------------\n",
      "Loss: 0.021085\n",
      "-------------- Epoch 15 --------------\n",
      "Loss: 0.012534\n",
      "-------------- Epoch 16 --------------\n",
      "Loss: 0.017626\n",
      "-------------- Epoch 17 --------------\n",
      "Loss: 0.020268\n",
      "-------------- Epoch 18 --------------\n",
      "Loss: 0.012202\n",
      "-------------- Epoch 19 --------------\n",
      "Loss: 0.016836\n",
      "-------------- Epoch 20 --------------\n",
      "Loss: 0.012295\n",
      "-------------- Epoch 21 --------------\n",
      "Loss: 0.012139\n",
      "-------------- Epoch 22 --------------\n",
      "Loss: 0.010920\n",
      "-------------- Epoch 23 --------------\n",
      "Loss: 0.005751\n",
      "-------------- Epoch 24 --------------\n",
      "Loss: 0.008626\n",
      "-------------- Epoch 25 --------------\n",
      "Loss: 0.010342\n",
      "-------------- Epoch 26 --------------\n",
      "Loss: 0.005872\n",
      "-------------- Epoch 27 --------------\n",
      "Loss: 0.005667\n",
      "-------------- Epoch 28 --------------\n",
      "Loss: 0.005234\n",
      "-------------- Epoch 29 --------------\n"
     ]
    }
   ],
   "source": [
    "for e in range(30):\n",
    "    train(model, device, train_loader, optimizer)\n",
    "    \n",
    "    acc_inf = test(model, device, test_loader_inf)\n",
    "    acc_sup = test(model, device, test_loader_sup)\n",
    "    \n",
    "    \n",
    "    print (f'-------------- Epoch {e} --------------')\n",
    "    # print (f'accuracy on inferior traces: {acc_inf:.2f}')\n",
    "    # print (f'accuracy on superior traces: {acc_sup:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7433035d-0422-46af-85d4-50371a63c423",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Following block shows the precision and recall of the model for different thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb9615bc-1161-4d39-8ab5-a46242f90d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.1, 1, 0.1)\n",
    "num_ow_test_samples = len(x_ow_test_sup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cf67aa-2c2b-437d-89b1-65658947b76a",
   "metadata": {},
   "source": [
    "### Inferior traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "842ae8b0-6023-46f3-8c91-1577934d6a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- threshold = 0.1\n",
      "Precision: 98.4, Recall: 42.4, F1 Score: 59.3\n",
      "--------------------- threshold = 0.2\n",
      "Precision: 98.7, Recall: 42.1, F1 Score: 59.0\n",
      "--------------------- threshold = 0.3\n",
      "Precision: 99.3, Recall: 40.0, F1 Score: 57.0\n",
      "--------------------- threshold = 0.4\n",
      "Precision: 99.6, Recall: 37.8, F1 Score: 54.8\n",
      "--------------------- threshold = 0.5\n",
      "Precision: 99.5, Recall: 32.7, F1 Score: 49.2\n",
      "--------------------- threshold = 0.6\n",
      "Precision: 99.6, Recall: 27.5, F1 Score: 43.1\n",
      "--------------------- threshold = 0.7\n",
      "Precision: 99.5, Recall: 22.4, F1 Score: 36.5\n",
      "--------------------- threshold = 0.8\n",
      "Precision: 100.0, Recall: 16.9, F1 Score: 28.9\n",
      "--------------------- threshold = 0.9\n",
      "Precision: 100.0, Recall: 11.1, F1 Score: 20.0\n"
     ]
    }
   ],
   "source": [
    "for th in thresholds:\n",
    "    print (f'--------------------- threshold = {th:.1f}')\n",
    "    P, R = test_ow(model, device, test_loader_inf, th, num_ow_test_samples)\n",
    "    print (f'Precision: {P*100:.1f}, Recall: {R*100:.1f}, F1 Score: {2*(P*R)*100/(P + R):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f286cd-141f-402b-acf0-7273575160d9",
   "metadata": {},
   "source": [
    "### Superior traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b941257-1d30-4b23-98fa-02f3ae02901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- threshold = 0.1\n",
      "Precision: 99.4, Recall: 66.7, F1 Score: 79.8\n",
      "--------------------- threshold = 0.2\n",
      "Precision: 99.4, Recall: 66.7, F1 Score: 79.8\n",
      "--------------------- threshold = 0.3\n",
      "Precision: 99.3, Recall: 65.8, F1 Score: 79.2\n",
      "--------------------- threshold = 0.4\n",
      "Precision: 99.7, Recall: 63.3, F1 Score: 77.4\n",
      "--------------------- threshold = 0.5\n",
      "Precision: 99.8, Recall: 59.2, F1 Score: 74.3\n",
      "--------------------- threshold = 0.6\n",
      "Precision: 100.0, Recall: 52.9, F1 Score: 69.2\n",
      "--------------------- threshold = 0.7\n",
      "Precision: 100.0, Recall: 45.9, F1 Score: 63.0\n",
      "--------------------- threshold = 0.8\n",
      "Precision: 100.0, Recall: 37.7, F1 Score: 54.8\n",
      "--------------------- threshold = 0.9\n",
      "Precision: 100.0, Recall: 26.4, F1 Score: 41.8\n"
     ]
    }
   ],
   "source": [
    "for th in thresholds:\n",
    "    print (f'--------------------- threshold = {th:.1f}')\n",
    "    P, R = test_ow(model, device, test_loader_sup, th, num_ow_test_samples)\n",
    "    print (f'Precision: {P*100:.1f}, Recall: {R*100:.1f}, F1 Score: {2*(P*R)*100/(P + R):.1f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.conda-pytorch-env)",
   "language": "python",
   "name": "conda-env-.conda-pytorch-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
