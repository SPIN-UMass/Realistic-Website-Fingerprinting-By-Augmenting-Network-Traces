{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e8ccefb-cc8c-4ff6-ae9b-54db19f997ea",
   "metadata": {},
   "source": [
    "# NetFM\n",
    "\n",
    "In this notebook, we implement NetFM, a WF attack based on semi-supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "339040c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "import pickle\n",
    "import argparse\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db87d03-453e-4d4e-9600-ce26b29adeaa",
   "metadata": {},
   "source": [
    "## GPU Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8063912d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\", 0)\n",
    "kwargs = {'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "print (f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a10c986-7df8-4c0f-8f7e-681937e05ba1",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "N defines the number of labeled samples per website.\n",
    "\n",
    "mu defines the ratio of unlabeled data to labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0de43f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "N = 5\n",
    "mu = 7\n",
    "num_epoches = 100\n",
    "\n",
    "data_path = '/path/to/AWF/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba29a032-75b5-4e14-8881-0c00558d6e06",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c0eb664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mon_test_ins 417\n",
      "test_feature 41700\n",
      "2081\n",
      "417\n",
      "Monitored training set partitioning...\n",
      "100\n",
      "2081\n",
      "Monitored testing set partitioning...\n",
      "train_feature:  208100\n",
      "train_label:  208100\n",
      "test_feature:  41700\n",
      "test_label:  41700\n",
      "train_dim:  5000\n",
      "test_dim:  5000\n"
     ]
    }
   ],
   "source": [
    "r_train = 500.0 / 6.0\n",
    "r_test = 100.0 / 6.0\n",
    "num_classes = 100\n",
    "mon_instance = 2498.0\n",
    "\n",
    "\n",
    "def split_awf_closed(r_train, r_test, nClass, mon_instance, dim):\n",
    "    mon_data = np.load(f'{data_path}/awf1.npz')\n",
    "    mon_x = mon_data['feature']\n",
    "    \n",
    "    num_mtrain_instance = mon_instance * (r_train / (r_train + r_test))\n",
    "    mon_random = np.array(range(int(mon_instance)))\n",
    "    np.random.shuffle(mon_random)\n",
    "    \n",
    "    mon_train_ins = mon_random[:int(num_mtrain_instance)] #1666\n",
    "    mon_test_ins = mon_random[int(num_mtrain_instance):]\n",
    "    print('mon_test_ins', len(mon_test_ins))\n",
    "    \n",
    "    train_feature = np.zeros((nClass*len(mon_train_ins), dim), dtype=int)\n",
    "    test_feature = np.zeros((nClass*len(mon_test_ins),dim), dtype=int)\n",
    "    print('test_feature', len(test_feature))\n",
    "    train_label = np.zeros((nClass*len(mon_train_ins),), dtype=int)\n",
    "    test_label = np.zeros((nClass*len(mon_test_ins),), dtype=int)\n",
    "\n",
    "    print(len(mon_train_ins))\n",
    "    print(len(mon_test_ins))\n",
    "    i = 0\n",
    "    mon_instance = int(mon_instance)\n",
    "    print('Monitored training set partitioning...')\n",
    "    print(nClass)\n",
    "    print(len(mon_train_ins))\n",
    "    for c in range(nClass):\n",
    "        c=int(c)\n",
    "        # print(c)\n",
    "        for instance in mon_train_ins:\n",
    "            train_label[i] = c\n",
    "            train_feature[i] = mon_x[(c*mon_instance)+instance][:dim]\n",
    "            i += 1\n",
    "\n",
    "    print('Monitored testing set partitioning...')\n",
    "    j = 0\n",
    "    for c in range(nClass):\n",
    "        c = int(c)\n",
    "        for instance in mon_test_ins:\n",
    "            test_label[j]=c\n",
    "            test_feature[j]=mon_x[(c*mon_instance)+instance][:dim]\n",
    "            j += 1\n",
    "\n",
    "    print('train_feature: ', len(train_feature))\n",
    "    print('train_label: ', len(train_label))\n",
    "    print('test_feature: ', len(test_feature))\n",
    "    print('test_label: ', len(test_label))\n",
    "    print('train_dim: ', len(train_feature[0]))\n",
    "    print('test_dim: ', len(test_feature[0]))\n",
    "\n",
    "\n",
    "    return train_feature, train_label, test_feature, test_label\n",
    "\n",
    "\n",
    "x_train, y_train, x_test, y_test = split_awf_closed(r_train, r_test, num_classes, mon_instance, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be556126-5327-4649-b197-0a27bcbd1274",
   "metadata": {},
   "source": [
    "## Randomly Sample Indices for Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cbf78d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_u_split():\n",
    "    labels = y_train\n",
    "    labeled_index = []\n",
    "    unlabeled_index = np.array(range(len(labels)))\n",
    "    for c in range(num_classes):\n",
    "        idx = np.where(labels == c)[0]\n",
    "        idx = np.random.choice(idx, N, False)\n",
    "        labeled_index.extend(idx)\n",
    "    labeled_index = np.array(labeled_index)\n",
    "    np.random.shuffle(labeled_index)\n",
    "    return labeled_index, unlabeled_index\n",
    "\n",
    "\n",
    "def x_u_split_mu(mu):\n",
    "    labels = y_train\n",
    "    labeled_index = []\n",
    "    unlabeled_index = []\n",
    "    for c in range(num_classes):\n",
    "        idx = np.where(labels == c)[0]\n",
    "        idx_u = np.random.choice(idx, N*(mu+1), False)\n",
    "        idx_l = idx_u[:N]\n",
    "        labeled_index.extend(idx_l)\n",
    "        unlabeled_index.extend(idx_u)\n",
    "    labeled_index = np.array(labeled_index)\n",
    "    unlabeled_index = np.array(unlabeled_index)\n",
    "    np.random.shuffle(labeled_index)\n",
    "    np.random.shuffle(unlabeled_index)\n",
    "    return labeled_index, unlabeled_index\n",
    "\n",
    "\n",
    "labeled_index, unlabeled_index = x_u_split_mu(mu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4993257b-f3fe-467b-88c3-0885b8880ce1",
   "metadata": {},
   "source": [
    "## DF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbfbfe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFNet(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super(DFNet, self).__init__()\n",
    "        kernel_size = 8\n",
    "        channels = [1, 32, 64, 128, 256]\n",
    "        conv_stride = 1\n",
    "        pool_stride = 4\n",
    "        pool_size = 8\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size, stride = conv_stride)\n",
    "        self.conv1_1 = nn.Conv1d(32, 32, kernel_size, stride = conv_stride)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size, stride = conv_stride)\n",
    "        self.conv2_2 = nn.Conv1d(64, 64, kernel_size, stride = conv_stride)\n",
    "       \n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size, stride = conv_stride)\n",
    "        self.conv3_3 = nn.Conv1d(128, 128, kernel_size, stride = conv_stride)\n",
    "       \n",
    "        self.conv4 = nn.Conv1d(128, 256, kernel_size, stride = conv_stride)\n",
    "        self.conv4_4 = nn.Conv1d(256, 256, kernel_size, stride = conv_stride)\n",
    "       \n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(32)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(64)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(128)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.max_pool_1 = nn.MaxPool1d(kernel_size=pool_size, stride=pool_stride)\n",
    "        self.max_pool_2 = nn.MaxPool1d(kernel_size=pool_size, stride=pool_stride)\n",
    "        self.max_pool_3 = nn.MaxPool1d(kernel_size=pool_size, stride=pool_stride)\n",
    "        self.max_pool_4 = nn.MaxPool1d(kernel_size=pool_size, stride=pool_stride)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.dropout2 = nn.Dropout(p=0.1)\n",
    "        self.dropout3 = nn.Dropout(p=0.1)\n",
    "        self.dropout4 = nn.Dropout(p=0.1)\n",
    "        \n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(5120, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.7),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(512, out_dim)\n",
    "\n",
    "        \n",
    "    def weight_init(self):\n",
    "        for n, m in self.named_modules():\n",
    "            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):\n",
    "                torch.nn.init.xavier_uniform(m.weight)\n",
    "                m.bias.data.zero_()\n",
    "            \n",
    "        \n",
    "    def forward(self, inp):\n",
    "        x = inp\n",
    "        # ==== first block ====\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.elu((self.conv1(x)))\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.elu(self.batch_norm1(self.conv1_1(x)))\n",
    "        x = F.pad(x, (3, 4))\n",
    "        x = self.max_pool_1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # ==== second block ====\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.relu((self.conv2(x)))\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.relu(self.batch_norm2(self.conv2_2(x)))\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = self.max_pool_2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # ==== third block ====\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.relu((self.conv3(x)))\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.relu(self.batch_norm3(self.conv3_3(x)))\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = self.max_pool_3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # ==== fourth block ====\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.relu((self.conv4(x)))\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = F.relu(self.batch_norm4(self.conv4_4(x)))\n",
    "        x = F.pad(x, (3,4))\n",
    "        x = self.max_pool_4(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "                \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.projection(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794bbfd5-160c-46a9-b729-2c4f3b87bc3c",
   "metadata": {},
   "source": [
    "## NetAugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64d21ba0-5709-4e9d-b3a5-98407bc16e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bursts(x):\n",
    "    \n",
    "    direction = x[0]\n",
    "    bursts = []\n",
    "    start = 0\n",
    "    temp_burst = x[0]\n",
    "    for i in range(1, len(x)):\n",
    "        if x[i] == 0.0:\n",
    "            break\n",
    "        \n",
    "        elif x[i] == direction:\n",
    "            temp_burst += x[i]\n",
    "            \n",
    "        else:\n",
    "            bursts.append((start, i, temp_burst))\n",
    "            start = i\n",
    "            temp_burst = x[i]\n",
    "            direction *= -1\n",
    "            \n",
    "    return bursts\n",
    "\n",
    "outgoing_burst_sizes = []\n",
    "\n",
    "x_random = x_train[np.random.choice(range(len(x_train)), size=1000, replace=False)]\n",
    "\n",
    "\n",
    "\n",
    "for x in x_random:\n",
    "    bursts = find_bursts(x)\n",
    "    \n",
    "    outgoing_burst_sizes += [x[2] for x in bursts if x[2] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab79e92-fa64-4aa1-95e4-5b24f25093ad",
   "metadata": {},
   "source": [
    "### Empirical Distribution of Outgoing Bursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb12c5c1-d79f-4d06-b31d-0d68dbc33f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count, bins = np.histogram(outgoing_burst_sizes, bins=77)\n",
    "PDF = count/np.sum(count)\n",
    "OUTGOING_BURST_SIZE_CDF = np.zeros_like(bins)\n",
    "OUTGOING_BURST_SIZE_CDF[1:] = np.cumsum(PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "622cd366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmentor():\n",
    "    def __init__(self):\n",
    "        methods = {\n",
    "            'merge downstream burst',\n",
    "            'change downstream burst sizes',\n",
    "            'merge downstream and upstream bursts',\n",
    "            'add upstream bursts',\n",
    "            'remove upstrean bursts',\n",
    "            'divide bursts'\n",
    "        }\n",
    "        \n",
    "        rules = {\n",
    "            'change content',\n",
    "            \n",
    "        }\n",
    "        \n",
    "        self.large_burst_threshold = 10\n",
    "        \n",
    "        # changing the content\n",
    "        self.upsample_rate = 1.0\n",
    "        self.downsample_rate = 0.5\n",
    "        \n",
    "        # merging bursts\n",
    "        self.num_bursts_to_merge = 5\n",
    "        self.merge_burst_rate = 0.1\n",
    "        \n",
    "        # add incoming bursts\n",
    "        self.add_outgoing_burst_rate = 0.3\n",
    "        self.outgoing_burst_sizes = list(range(73))\n",
    "        \n",
    "        # shift\n",
    "        self.shift_param = 10\n",
    "        \n",
    "        \n",
    "        \n",
    "    def find_bursts(self, x):\n",
    "        direction = x[0]\n",
    "        bursts = []\n",
    "        start = 0\n",
    "        temp_burst = x[0]\n",
    "        for i in range(1, len(x)):\n",
    "            if x[i] == 0.0:\n",
    "                break\n",
    "\n",
    "            elif x[i] == direction:\n",
    "                temp_burst += x[i]\n",
    "\n",
    "            else:\n",
    "                bursts.append((start, i, temp_burst))\n",
    "                start = i\n",
    "                temp_burst = x[i]\n",
    "                direction *= -1\n",
    "\n",
    "        return bursts\n",
    "        \n",
    "        \n",
    "    # representing the change of website content\n",
    "    def increase_incoming_bursts(self, burst_sizes):\n",
    "        out = []\n",
    "        for i, size in enumerate(burst_sizes):\n",
    "            if size <= -self.large_burst_threshold:\n",
    "                up_sample_rate = random.random()*self.upsample_rate\n",
    "                new_size = int(size * (1+up_sample_rate))\n",
    "                out.append(new_size)\n",
    "            else:\n",
    "                out.append(size)\n",
    "                \n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def decrease_incoming_bursts(self, burst_sizes):\n",
    "        out = []\n",
    "        for i, size in enumerate(burst_sizes):\n",
    "            if size <= -self.large_burst_threshold:\n",
    "                up_sample_rate = random.random()*self.downsample_rate\n",
    "                new_size = int(size * (1-up_sample_rate))\n",
    "                out.append(new_size)\n",
    "            else:\n",
    "                out.append(size)\n",
    "                \n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def change_content(self, trace):\n",
    "        bursts = self.find_bursts(trace)\n",
    "        burst_sizes = [x[2] for x in bursts]\n",
    "        \n",
    "        if len(trace) < 1000:\n",
    "            new_burst_sizes = self.increase_incoming_bursts(burst_sizes)\n",
    "            \n",
    "        elif len(trace) > 4000:\n",
    "            new_burst_sizes = self.decrease_incoming_bursts(burst_sizes)\n",
    "            \n",
    "        else:\n",
    "            p = random.random()\n",
    "            if p >= 0.5:\n",
    "                new_burst_sizes = self.increase_incoming_bursts(burst_sizes)\n",
    "                \n",
    "            else:\n",
    "                new_burst_sizes = self.decrease_incoming_bursts(burst_sizes)\n",
    "                \n",
    "                \n",
    "        return new_burst_sizes\n",
    "    \n",
    "    \n",
    "    def merge_incoming_bursts(self, burst_sizes):\n",
    "        \n",
    "        out = []\n",
    "        \n",
    "        # skipping first 20 cells\n",
    "        i = 0\n",
    "        num_cells = 0\n",
    "        while i < len(burst_sizes) and num_cells < 20:\n",
    "            num_cells += abs(burst_sizes[i])\n",
    "            out.append(burst_sizes[i])\n",
    "            i += 1\n",
    "            \n",
    "        \n",
    "        while i < len(burst_sizes) - self.num_bursts_to_merge:\n",
    "            prob = random.random()\n",
    "            \n",
    "            # ignore outgoing bursts\n",
    "            if burst_sizes[i] > 0:\n",
    "                out.append(burst_sizes[i])\n",
    "                i+= 1\n",
    "                continue\n",
    "            \n",
    "            if prob < self.merge_burst_rate:\n",
    "                num_merges = random.randint(2, self.num_bursts_to_merge)\n",
    "                merged_size = 0\n",
    "                \n",
    "                # merging the incoming bursts\n",
    "                while i < len(burst_sizes) and num_merges > 0:\n",
    "                    if burst_sizes[i] < 0:\n",
    "                        merged_size += burst_sizes[i]\n",
    "                        num_merges -= 1\n",
    "                    i += 1     \n",
    "                out.append(merged_size)\n",
    "                    \n",
    "            else:\n",
    "                out.append(burst_sizes[i])\n",
    "                i += 1\n",
    "                \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def add_outgoing_burst(self, burst_sizes):\n",
    "        \n",
    "        out = []\n",
    "        \n",
    "        i = 0\n",
    "        num_cells = 0\n",
    "        while i < len(burst_sizes) and num_cells < 20:\n",
    "            num_cells += abs(burst_sizes[i])\n",
    "            out.append(burst_sizes[i])\n",
    "            i += 1\n",
    "            \n",
    "        \n",
    "        for size in burst_sizes[i:]:\n",
    "            if size > -10 :\n",
    "                out.append(size)\n",
    "                continue\n",
    "            \n",
    "            prob = random.random()\n",
    "            \n",
    "            if prob < self.add_outgoing_burst_rate:\n",
    "                outgoing_burst_prob = random.random()\n",
    "                index = bisect.bisect_left(OUTGOING_BURST_SIZE_CDF, outgoing_burst_prob)\n",
    "                # print (index)\n",
    "                outgoing_burst_size = self.outgoing_burst_sizes[index]\n",
    "                # print (outgoing_burst_size)\n",
    "                divide_place = random.randint(3, abs(size) - 3)\n",
    "                \n",
    "                out += [-divide_place, outgoing_burst_size, -(abs(size) - divide_place)]\n",
    "                \n",
    "            else:\n",
    "                out.append(size)\n",
    "                \n",
    "        return out\n",
    "                \n",
    "        \n",
    "    def create_trace_from_burst_sizes(self, burst_sizes):\n",
    "        out = []\n",
    "        \n",
    "        for size in burst_sizes:\n",
    "            val = 1 if size > 0 else -1\n",
    "            \n",
    "            out += [val]*(int(abs(size)))\n",
    "            \n",
    "        if len(out) < 5000:\n",
    "            out += [0]*(5000 - len(out))\n",
    "            \n",
    "        return np.array(out)[:5000]\n",
    "    \n",
    "    def shift(self, x):\n",
    "        pad = np.random.randint(0, 2, size = (self.shift_param, ))\n",
    "        pad = 2*pad-1\n",
    "        zpad = np.zeros_like(pad)\n",
    "        \n",
    "        shift_val = np.random.randint(-self.shift_param, self.shift_param+1, 1)[0]\n",
    "        shifted = np.concatenate((x, zpad, pad), axis=-1)\n",
    "        shifted = np.roll(shifted, shift_val, axis=-1)\n",
    "        shifted = shifted[:5000]\n",
    "        \n",
    "        return shifted\n",
    "        \n",
    "    \n",
    "    def augment(self, trace):\n",
    "        \n",
    "        mapping = {\n",
    "            0: self.change_content,\n",
    "            1: self.merge_incoming_bursts,\n",
    "            2: self.add_outgoing_burst\n",
    "        }\n",
    "        \n",
    "        bursts = self.find_bursts(trace)\n",
    "        \n",
    "        burst_sizes = [x[2] for x in bursts]\n",
    "        \n",
    "        \n",
    "        aug_method = mapping[random.randint(0, len(mapping)-1)]\n",
    "        \n",
    "        augmented_sizes = aug_method(burst_sizes)\n",
    "        \n",
    "        augmented_trace = self.create_trace_from_burst_sizes(augmented_sizes)\n",
    "        \n",
    "        return self.shift(augmented_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d7723a-576f-4f11-9f3a-c8e9135a55f6",
   "metadata": {},
   "source": [
    "## Weak Augmentor: Random Direction Flips\n",
    "\n",
    "For weak augmentation in NetFM, we just randomly flip the direction of Tor cells with 0.1 probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89c4de62-789b-4d44-b6c9-9df6b507627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeakAugmentor():\n",
    "    def __init__(self, param):\n",
    "        self.param = param\n",
    "        \n",
    "    def augment(self, x):\n",
    "        z = np.random.uniform(size=(5000, ))\n",
    "        \n",
    "        noised = np.where(z > self.param, x, -x)\n",
    "        \n",
    "        return noised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16936ec8-a626-4d0e-a381-7acbc95d84cc",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a3ad7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledData(Dataset):\n",
    "    def __init__(self, x_train, y_train, idx, weak_augmentor):\n",
    "        self.x = x_train[idx]\n",
    "        self.y = y_train[idx]\n",
    "        self.weak_augmentor = weak_augmentor\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.weak_augmentor.augment(self.x[index]), self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "class TestData(Dataset):\n",
    "    def __init__(self, x_test, y_test):\n",
    "        self.x = x_test\n",
    "        self.y = y_test\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "class ValidData(Dataset):\n",
    "    def __init__(self, x_valid, y_valid):\n",
    "        self.x = x_valid\n",
    "        self.y = y_valid\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "class UnLabeledData(Dataset):\n",
    "    def __init__(self, x_train, idx, weak_augmentor, strong_augmentor):\n",
    "        self.x = x_train[idx]\n",
    "        self.weak_augmentor = weak_augmentor\n",
    "        self.strong_augmentor = strong_augmentor    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.weak_augmentor.augment(self.x[index]), self.strong_augmentor.augment(self.x[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f00e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_augmentor = WeakAugmentor(0.1)\n",
    "strong_augmentor = Augmentor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cefb19c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = LabeledData(x_train, y_train, labeled_index, weak_augmentor)\n",
    "labeled_loader = DataLoader(labeled_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "unlabeled_data = UnLabeledData(x_train, unlabeled_index, weak_augmentor, strong_augmentor)\n",
    "unlabeled_loader = DataLoader(unlabeled_data, batch_size=(mu+1)*batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "test_data = TestData(x_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2bffc0-f6bf-4248-bbc2-c8a333e41a3a",
   "metadata": {},
   "source": [
    "## NetFM Train Function\n",
    "\n",
    "In the NetFM train function, we generate pseudo labels using weakly augmented traces. We then use the pseudo labels as the acutal label of strongly augmented traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74880a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, labeled_loader, unlabeled_loader, optimizer, schedular):\n",
    "    model.train()\n",
    "    labeled_iterator = iter(labeled_loader)\n",
    "    for batch_idx, (x_w, x_s) in enumerate(unlabeled_loader):\n",
    "        try:\n",
    "            x_l, target = next(labeled_iterator)\n",
    "        except StopIteration:\n",
    "            labeled_iterator = iter(labeled_loader)\n",
    "            x_l, target = next(labeled_iterator)\n",
    "        \n",
    "        x_w = x_w.view(x_w.size(0), 1, x_w.size(1)).float().to(device)\n",
    "        x_s = x_s.view(x_s.size(0), 1, x_s.size(1)).float().to(device)\n",
    "        x_l = x_l.view(x_l.size(0), 1, x_l.size(1)).float().to(device)\n",
    "        \n",
    "        target = target.to(device)\n",
    "\n",
    "        inputs = torch.cat((x_l, x_w, x_s))\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)\n",
    "        logits_x = logits[:batch_size]\n",
    "\n",
    "        logits_w, logits_s = logits[batch_size:].chunk(2)\n",
    "\n",
    "        del logits\n",
    "\n",
    "        Lx = F.cross_entropy(logits_x, target)\n",
    "\n",
    "        pseudo_label = torch.softmax(logits_w.detach()/T, dim=-1)\n",
    "        max_probs, targets_u = torch.max(pseudo_label, dim=-1)\n",
    "        mask = max_probs.ge(threshold).float()\n",
    "        \n",
    "        Lu = (F.cross_entropy(logits_s, targets_u, reduction='none')*mask).mean()\n",
    "\n",
    "        loss = Lx + lambda_u*Lu\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        schedular.step()\n",
    "        \n",
    "\n",
    "    return (Lx.item(), Lu.item(), loss.item())\n",
    "\n",
    "        \n",
    "        \n",
    "def test(model, device, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data = data.view(data.size(0), 1, data.size(1)).float().to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            output = torch.softmax(output, dim=1)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).float().sum().item()\n",
    "\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1dd37c-de77-4921-9d75-5c776cbeae02",
   "metadata": {},
   "source": [
    "## Cosine Scheduler\n",
    "\n",
    "According to original implementation of FixMatch, we add a cosine scheduler to NetFM optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b31b107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_schedule_with_warmup(optimizer,\n",
    "                                    num_warmup_steps,\n",
    "                                    num_training_steps,\n",
    "                                    num_cycles=7./16.,\n",
    "                                    last_epoch=-1):\n",
    "    \n",
    "    def _lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        no_progress = float(current_step - num_warmup_steps) / \\\n",
    "            float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0., math.cos(math.pi * num_cycles * no_progress))\n",
    "\n",
    "    return LambdaLR(optimizer, _lr_lambda, last_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90c7fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DFNet(out_dim=num_classes).to(device)\n",
    "model.weight_init()\n",
    "optimizer = optim.SGD(model.parameters(),lr = 0.01, momentum=0.9, nesterov=True)\n",
    "schedular = get_cosine_schedule_with_warmup(optimizer, 0, num_epoches*(len(unlabeled_data)/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76e85ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1\n",
    "threshold = 0.95\n",
    "lambda_u = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f52cee33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/101 [00:19<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 101\n",
    "best_acc = 0\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "    lx, lu, loss = train(model, device, labeled_loader, unlabeled_loader, optimizer, schedular)\n",
    "    # acc = test(model, device, test_loader)\n",
    "    print ('======== Epoch %d ========'%epoch)\n",
    "    print ('Lx: ', lx, 'Lu: ', lu, 'loss: ', loss)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
